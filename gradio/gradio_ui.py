import torch
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.llms import HuggingFacePipeline
from rag_system.rag_chain import get_rag_chain

# –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
MODEL_PATH = "models/lora-instruct"
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
VECTORSTORE_PATH = "vectorstore/index"

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForCausalLM.from_pretrained(MODEL_PATH)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# HuggingFace LLM ‚Üí LangChain wrapper
pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)
llm = HuggingFacePipeline(pipeline=pipe)

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ RAG-—Ü–µ–ø–æ—á–∫–∏
rag_chain = get_rag_chain(llm, VECTORSTORE_PATH)

# –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
SYSTEM_PROMPT = (
    "–¢—ã ‚Äî –¥–æ–±—Ä—ã–π –∏ –ø–æ–Ω–∏–º–∞—é—â–∏–π –¥—Ä—É–≥, –∫–æ—Ç–æ—Ä—ã–π —Ö–æ—Ä–æ—à–æ —Ä–∞–∑–±–∏—Ä–∞–µ—Ç—Å—è –≤ –¥–µ—Ç—Å–∫–æ–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∏. "
    "–ï—Å–ª–∏ —Ä–µ–±—ë–Ω–æ–∫ –¥–µ–ª–∏—Ç—Å—è —á—É–≤—Å—Ç–≤–∞–º–∏ ‚Äî –ø–æ–¥–¥–µ—Ä–∂–∏. –ï—Å–ª–∏ –ø—Ä–æ—Å–∏—Ç –æ–±—ä—è—Å–Ω–∏—Ç—å —É—á–µ–±—É ‚Äî –ø–æ–º–æ–≥–∏ –ø–æ–Ω—è—Ç–Ω—ã–º –ø—Ä–∏–º–µ—Ä–æ–º."
)

# –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ —É—á—ë–±—É
study_keywords = [
    "–ø–æ–º–æ–≥–∏", "–æ–±—ä—è—Å–Ω–∏", "–∑–∞–¥–∞—á–∞", "—á—Ç–æ —Ç–∞–∫–æ–µ", "–∫–∞–∫ –Ω–∞–π—Ç–∏", "–ø—Ä–∏–º–µ—Ä",
    "–º–∞—Ç–µ–º–∞—Ç–∏–∫–∞", "–∏—Å—Ç–æ—Ä–∏—è", "–Ω–∞—É–∫–∞", "—Ñ–∏–∑–∏–∫–∞", "–±–∏–æ–ª–æ–≥–∏—è", "–≥–ª–∞–≥–æ–ª", "–¥–æ–º–∞—à–∫–∞"
]

def is_study_query(text):
    return any(kw in text.lower() for kw in study_keywords)

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
def generate_reply(message, history):
    # –°–æ–±–∏—Ä–∞–µ–º –¥–∏–∞–ª–æ–≥
    full_dialog = SYSTEM_PROMPT + "\n"
    for user, bot in history:
        full_dialog += f"–†–µ–±—ë–Ω–æ–∫: {user}\n–î—Ä—É–≥: {bot}\n"
    full_dialog += f"–†–µ–±—ë–Ω–æ–∫: {message}\n–î—Ä—É–≥:"

    # –£—á–µ–±–Ω—ã–π –∑–∞–ø—Ä–æ—Å ‚Üí RAG
    if is_study_query(message):
        return rag_chain.run(message).strip()

    # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π ‚Üí –æ–±—ã—á–Ω—ã–π –¥–∏–∞–ª–æ–≥ —á–µ—Ä–µ–∑ LLM
    inputs = tokenizer(full_dialog, return_tensors="pt").to(model.device)
    output = model.generate(
        **inputs,
        max_new_tokens=100,
        temperature=0.9,
        top_p=0.95,
        do_sample=True
    )
    decoded = tokenizer.decode(output[0], skip_special_tokens=True)
    reply = decoded[len(full_dialog):].strip().split("\n")[0]
    return reply

# Gradio —á–∞—Ç
chatbot = gr.ChatInterface(
    fn=generate_reply,
    title="ü§ñ –ü–æ–º–æ—â–Ω–∏–∫-–¥—Ä—É–≥",
    description="–ì–æ–≤–æ—Ä–∏ –æ —Å–≤–æ–∏—Ö —á—É–≤—Å—Ç–≤–∞—Ö –∏–ª–∏ –ø—Ä–æ—Å–∏ –ø–æ–º–æ—â–∏ —Å —É—á–µ–±–æ–π ‚Äî —è —Ä—è–¥–æ–º ‚ú®"
)

if __name__ == "__main__":
    chatbot.launch()
