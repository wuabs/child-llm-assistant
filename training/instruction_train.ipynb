{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/wuabs/child-llm-assistant/blob/main/training/instruction_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1FwBiyVwe_I"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "from datasets import load_dataset\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3f-xfZH7wvcp"
   },
   "outputs": [],
   "source": [
    "# Загружаем дообученную модель\n",
    "model_path = \"./models/lora-lm\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, load_in_8bit=True, device_map=\"auto\")\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mCNeQD0ywyYV"
   },
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "with open(\"lora_config.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    config_dict = json.load(f)\n",
    "lora_config = LoraConfig(**config_dict)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPzSy4Scw2C-"
   },
   "outputs": [],
   "source": [
    "# Датасет\n",
    "dataset = load_dataset(\"json\", data_files=\"data/child_dialogs.jsonl\", split=\"train\")\n",
    "\n",
    "def tokenize(example):\n",
    "    prompt = f\"### Инструкция:\\\\n{example['child_input']}\\\\n\\\\n### Ответ:\\\\n{example['friend_reply']}\"\n",
    "    return tokenizer(prompt, truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q0FTHB4Lw8B5"
   },
   "outputs": [],
   "source": [
    "# Аргументы\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/lora-instruct\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=20,\n",
    "    save_steps=200,\n",
    "    fp16=True,\n",
    "    save_total_limit=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yOQNpRTQw-TR"
   },
   "outputs": [],
   "source": [
    "# Обучение\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYQFRIH6xC6K"
   },
   "outputs": [],
   "source": [
    "# Сохраняем модель\n",
    "model.save_pretrained(\"./models/lora-instruct\")\n",
    "tokenizer.save_pretrained(\"./models/lora-instruct\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMhZv5VwnYvHlRZet+ymrPH",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
